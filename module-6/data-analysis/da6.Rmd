---
title: "BIOST 2049 - Data Analysis #6"
author: "Matthew Ragoza"
date: "4/7/2022"
always_allow_html: true
output:
  html_document:
    df_print: paged
    toc: true
    number_sections: false
    toc_depth: '3'
    code_folding: hide
  github_document:
    toc: true
---

# Physician office visits data set

The data set in this analysis was sourced from a study on the effects of demographics, health conditions, and insurance information on the frequency of doctor visits. The sample size included in the study was 2,500 people in the US. For this analysis, the outcome variable of interest is the number of physician office visits in the past two years.

```{r warning=FALSE, message=FALSE}
library('dplyr')
library('reshape2')
library('ggplot2')
library('gridExtra')
library('scales')
library('stats')
requireNamespace('glmnet')
requireNamespace('car')
requireNamespace('lrtest')
library('conflicted')

setwd('C:\\Users\\mtr22\\Code\\BIOST2049\\module-6\\data-analysis')
data <- read.csv('da6.csv')

convert_vars = function(data) {
  
  # convert categorical variables to factors
  data$health = factor(data$health, levels=1:3, labels=c('poor', 'avg', 'excel'))
  data$sex = factor(data$sex, levels=0:1, labels=c('F', 'M'))
  data$adldiff = factor(data$adldiff, levels=0:1, labels=c('no', 'yes'))
  data$race = factor(data$race, levels=0:1, labels=c('not AA', 'AA'))
  data$privins = factor(data$privins, level=0:1, labels=c('yes', 'no'))
  
  data$age = data$age * 10 # convert age to years
  
  # reinsert visit so it appears last
  x = data$visit
  data = subset(data, select=-visit)
  data$visit = x
  return(data)
}

data = convert_vars(data)
data
```

## Categorical variables

Five categorical variables were included in the data set, which were self-reported health condition, sex, presence of disability limiting daily living, race, and whether the subject had private health insurance. The self-reported health condition was coded as a three-level categorical variables with 13.6% of the sample reporting poor health, 79.6% reporting average health, and 6.9% reporting excellent health. The sample was 60.9% female and 39.1% male. 20.7% of the participants had a limiting disability. Race was coded as binary indicator such that 90.0% of the sample were not African American and 10% were. Furthermore, 80.1% of the sample had private health insurance and the remainder did not. 

```{r}
data %>%
  group_by(health) %>%
  summarize(count=n(), .groups='drop_last') %>%
  mutate(proportion=count/sum(count)) %>%
  mutate_if(is.double, percent_format(accuracy=0.1))
```

```{r}
data %>%
  group_by(sex) %>%
  summarize(count=n(), .groups='drop_last') %>%
  mutate(proportion=count/sum(count)) %>%
  mutate_if(is.double, percent_format(accuracy=0.1))
```

```{r}
data %>%
  group_by(adldiff) %>%
  summarize(count=n(), .groups='drop_last') %>%
  mutate(proportion=count/sum(count)) %>%
  mutate_if(is.double, percent_format(accuracy=0.1))
```

```{r}
data %>%
  group_by(race) %>%
  summarize(count=n(), .groups='drop_last') %>%
  mutate(proportion=count/sum(count)) %>%
  mutate_if(is.double, percent_format(accuracy=0.1))
```

```{r}
data %>%
  group_by(privins) %>%
  summarize(count=n(), .groups='drop_last') %>%
  mutate(proportion=count/sum(count)) %>%
  mutate_if(is.double, percent_format(accuracy=0.1))
```

```{r, fig.width=8, fig.height=3.5, dpi=100, warning=FALSE, message=FALSE}

blank <- ggplot() + theme_void()

red = 'orchid'
green = 'mediumseagreen'
blue = 'cornflowerblue'

# visit
y_scale = scale_y_continuous(trans='log10')

# histograms

plot_x1 <- data %>%
  ggplot(aes(x=health)) +
  geom_bar(fill=green) +
  theme_classic() +
  theme(panel.grid.major=element_line()) +
  xlab(NULL)

plot_x2 <- data %>%
  ggplot(aes(x=sex)) +
  geom_bar(fill=green) +
  theme_classic() +
  theme(panel.grid.major=element_line()) +
  xlab(NULL) +
  ylab(NULL)

plot_x3 <- data %>%
  ggplot(aes(x=adldiff)) +
  geom_bar(fill=green) +
  theme_classic() +
  theme(panel.grid.major=element_line()) +
  xlab(NULL) +
  ylab(NULL)

plot_x4 <- data %>%
  ggplot(aes(x=race)) +
  geom_bar(fill=green) +
  theme_classic() +
  theme(panel.grid.major=element_line()) +
  xlab(NULL) +
  ylab(NULL)

plot_x5 <- data %>%
  ggplot(aes(x=privins)) +
  geom_bar(fill=green) +
  theme_classic() +
  theme(panel.grid.major=element_line()) +
  xlab(NULL) +
  ylab(NULL)

plot_y <- data %>%
  ggplot(aes(y=visit)) +
  geom_histogram(fill=red, binwidth=0.2) +
  theme_classic() +
  theme(panel.grid.major=element_line()) +
  y_scale +
  ylab(NULL)

# scatter plots

plot_yx1 <- data %>%
  ggplot(aes(x=health, y=visit)) +
  geom_boxplot(fill=blue) +
  theme_classic() +
  theme(panel.grid.major=element_line(), legend.position='none') +
  y_scale

plot_yx2 <- data %>%
  ggplot(aes(x=sex, y=visit)) +
  geom_boxplot(fill=blue) +
  theme_classic() +
  theme(panel.grid.major=element_line(), legend.position='none') +
  y_scale +
  ylab(NULL)

plot_yx3 <- data %>%
  ggplot(aes(x=adldiff, y=visit)) +
  geom_boxplot(fill=blue) +
  theme_classic() +
  theme(panel.grid.major=element_line(), legend.position='none') +
  y_scale +
  ylab(NULL)

plot_yx4 <- data %>%
  ggplot(aes(x=race, y=visit)) +
  geom_boxplot(fill=blue) +
  theme_classic() +
  theme(panel.grid.major=element_line(), legend.position='none') +
  y_scale +
  ylab(NULL)

plot_yx5 <- data %>%
  ggplot(aes(x=privins, y=visit)) +
  geom_boxplot(fill=blue) +
  theme_classic() +
  theme(panel.grid.major=element_line(), legend.position='none') +
  y_scale +
  ylab(NULL)

# arrange plots on grid

grid.arrange(
    plot_x1,  plot_x2,  plot_x3,  plot_x4,  plot_x5,  blank,
    plot_yx1, plot_yx2, plot_yx3, plot_yx4, plot_yx5, plot_y,
    ncol=6, widths=c(3, 2, 2, 2, 2, 2),
    nrow=2, heights=c(1, 2)
)
```

## Quantitative variables

There were four quantitative variables in the data set, including the outcome variable, which was the number of physician office visits in the past two years. The other quantitative covariates were age (originally in decades, but I rescaled to years), number of chronic health conditions, and number of years of education. The age range of the participants was 66 to 102 with a median of 73. They had between 0 and 8 chronic health conditions, with a median of 1. The median number of years of education was 12, but the range was 0 to 18. We will examine the outcome variable in more detail in the next section.

```{r}
data %>%
  mutate(id=1:n()) %>%
  group_by(id) %>%
  select_if(is.numeric) %>%
  melt(id.vars='id') %>%
  group_by(variable) %>%
  summarize(
    count=n(),
    mean=mean(value),
    std=sd(value),
    min=min(value),
    Q1=quantile(value, 0.25),
    Q2=quantile(value, 0.50),
    Q3=quantile(value, 0.75),
    max=max(value)
  ) %>%
  mutate_if(is.double, round, digits=2)
```

```{r, fig.width=8.5, fig.height=3.5, warning=FALSE}

# grid settings

# age
x6_breaks = seq(65, 105, 5)
x6_scale = scale_x_continuous(limits=c(65, 105))

# cond
x7_breaks = seq(-0.5, 8.5, 1)
x7_scale = scale_x_continuous(limits=c(-0.5, 8.5))

# edu
x8_breaks = seq(-1, 19, 2)
x8_scale = scale_x_continuous(limits=c(-1, 19))

# histograms

plot_x6 <- data %>%
  ggplot(aes(x=age)) +
  geom_histogram(fill=green, breaks=x6_breaks) +
  theme_classic() +
  theme(panel.grid.major=element_line()) +
  xlab(NULL) +
  x6_scale

plot_x7 <- data %>%
  ggplot(aes(x=cond)) +
  geom_histogram(fill=green, breaks=x7_breaks) +
  theme_classic() +
  theme(panel.grid.major=element_line()) +
  xlab(NULL) +
  ylab(NULL) +
  x7_scale

plot_x8 <- data %>%
  ggplot(aes(x=edu)) +
  geom_histogram(fill=green, breaks=x8_breaks) +
  theme_classic() +
  theme(panel.grid.major=element_line()) +
  xlab(NULL) +
  ylab(NULL) +
  x8_scale

# scatter plots

plot_yx6 <- data %>%
  ggplot(aes(x=age, y=visit)) +
  geom_point(color=blue, alpha=0.1) +
  theme_classic() +
  theme(panel.grid.major=element_line(), legend.position='none') +
  x6_scale +
  y_scale

plot_yx7 <- data %>%
  ggplot(aes(x=cond, y=visit)) +
  geom_point(color=blue, alpha=0.1) +
  theme_classic() +
  theme(panel.grid.major=element_line(), legend.position='none') +
  ylab(NULL) +
  x7_scale + 
  y_scale

plot_yx8 <- data %>%
  ggplot(aes(x=edu, y=visit)) +
  geom_point(color=blue, alpha=0.1) +
  theme_classic() +
  theme(panel.grid.major=element_line(), legend.position='none') +
  ylab(NULL) + 
  x8_scale +
  y_scale

# arrange plots on grid

grid.arrange(
    plot_x6,  plot_x7,  plot_x8, blank,
    plot_yx6, plot_yx7, plot_yx8,   plot_y,
    ncol=4, widths=c(2, 2, 2, 1),
    nrow=2, heights=c(1, 2)
)
```

## Part 1 - Number of doctor visits

As seen in the previous table, the median number of doctor visits over the past two years in this sample was 5. However, the range was from 0 to 89, and the first and third quartile were 2 and 9. These descriptive statistics imply an extremely right-skewed distribution.

```{r, fig.width=8.5, fig.height=2.5}
data %>%
  ggplot(aes(x=visit)) +
  geom_histogram(fill=red, binwidth=1) +
  theme_classic() +
  theme(panel.grid.major=element_line())
```

As can be seen from the histogram above, the distribution of the number of doctor visits is highly right-skewed. Furthermore, the domain contains only non-negative integers, since it represents a form of count data. These observations contradict the assumptions of linear regression, which relies on normality in the conditional expectation of the outcome variable, and allows for negative and non-integer values. A more appropriate model would be Poisson regression, which is able to model the conditional expectation of a Poisson random variable. The support of the Poisson distribution are the non-negative integers, which represent the number of occurences of an event (e.g. doctor visits) in a given time interval.

## Part 2 - Poisson regression model

We will now fit a Poisson regression model to predict the number of doctor visits in the past two years. We will include all 8 available covariates as predictors in the model. Then we will check the model assumptions, goodness-of-fit, and assess the estimated coefficients.

```{r}
full_model = glm(
  visit ~ health + sex + adldiff + race + privins + age + cond + edu,
  data, family='poisson'
)
full_model
```
The assumptions behind the Poisson regression model are that the conditional distribution of the outcome variable follows a Poisson distribution, that the variance of the outcome equals the mean (conditional on the predictors), that the log of the outcome variable is a linear function of the predictors, and that the data are independently sampled. We can assume from the study design that the participants were independently sampled. The rest of the assumptions can be assessed by viewing the diagnostic plots below.

```{r, fig.width=8.5, fig.height=3.75, warning=FALSE}

predicted = predict(full_model, type='response')
residual = residuals(full_model, type='pearson')

# grid settings

p_breaks = 10^((0:24)/8)
p_scale = scale_x_continuous(limits=c(1, 100), trans='log10')

y_breaks = 10^((0:24)/8)
y_scale = scale_y_continuous(limits=c(1, 100), trans='log10')

p_breaks2 = seq(1, 25, 2.5)
p_scale2 = scale_x_continuous(limits=c(1, 25))

r_breaks = seq(-5, 30, 2)
r_scale = scale_y_continuous(limits=c(-5, 30))

# histograms

plot_p <- data %>%
  ggplot(aes(x=predicted)) +
  geom_histogram(fill=green, breaks=p_breaks) +
  theme_classic() +
  theme(panel.grid.major=element_line()) +
  xlab(NULL) +
  p_scale

plot_y <- data %>%
  ggplot(aes(y=visit)) +
  geom_histogram(fill=red, breaks=y_breaks) +
  theme_classic() +
  theme(panel.grid.major=element_line()) +
  ylab(NULL) +
  y_scale + scale_x_continuous(breaks=c(0, 1000, 2000))


plot_p2 <- data %>%
  ggplot(aes(x=predicted)) +
  geom_histogram(fill=green, breaks=p_breaks2) +
  theme_classic() +
  theme(panel.grid.major=element_line()) +
  xlab(NULL) +
  p_scale2

plot_r <- data %>%
  ggplot(aes(y=residual)) +
  geom_histogram(fill=red, breaks=r_breaks) +
  theme_classic() +
  theme(panel.grid.major=element_line()) +
  ylab(NULL) +
  r_scale

# scatter plots

plot_yp <- data %>%
  ggplot(aes(x=predicted, y=visit)) +
  geom_point(color=blue, alpha=0.1) +
  geom_smooth(method='lm', color='black') +
  theme_classic() +
  theme(panel.grid.major=element_line(), legend.position='none') +
  p_scale + y_scale

plot_rp <- data %>%
  ggplot(aes(x=predicted, y=residual)) +
  geom_point(color=blue, alpha=0.1) +
  geom_smooth(method='lm', color='black') +
  theme_classic() +
  theme(panel.grid.major=element_line(), legend.position='none') +
  p_scale2 + r_scale

# arrange plots on grid

grid.arrange(
    plot_p,  blank,  plot_p2,  blank,
    plot_yp, plot_y, plot_rp, plot_r,
    ncol=4, widths=c(2, 1, 2, 1),
    nrow=2, heights=c(1, 2)
)
```

The left plot above shows the true outcome plotted against the predicted outcome on a log scale, with a linear model. The slope of the relationship is very nearly identity. The model failed to predict any extremely high numbers of doctor visits (i.e. above 25), even though these were present in the true data. However, there is a moderate correlation for the bulk of the data. This would support the choice of the Poisson regression model by validating the assumption that the outcome is log-linearly related to the predictors. 

In the plot on the right, the Pearson residuals are plotted with respect to the predicted outcomes with another linear model, this time using linearly scaled axes. The marginal distribution of the Pearson residuals appears somewhat skewed, which is concerning. The Pearson residuals are normalized by the conditional variance, which in a Poisson model is simply the conditional expectation (i.e. predicted value). The residuals are centered at zero, but there are more high outliers than low outliers. We would expect the Pearson residuals to have constant variance, and they should ideally be much smaller in scale if the model is fit well. 

```{r}
# perform chi-squared gof test
chi2 = sum(residuals(full_model, type='pearson')**2)
df = full_model$df.residual
pchisq(chi2, df=df, lower.tail=FALSE)
```
We can perform a Pearson chi-squared goodness-of-fit test on the model. The null hypothesis is that the predicted counts followed the same distribution as the observed counts of doctor visits. The test statistic is simply the sum of the squared Pearson residuals, and the degrees of freedom is $n - k - 1$. We observed a p-value of 0, indicating that there is no chance of observing a test statistic this large if the null hypothesis is true. Therefore we reject the null hypothesis and conclude that the predicted counts are not from the same distribution as the observed counts.

One caveat for this test is that the counts only follow a chi-squared distribution asymptotically as the sample size increases, and if there are observed cell counts of zero then this assumption may not be valid. Overall it is difficult to say so far whether the Poisson model fits this data set.

## Part 3 - Assessing univariate models

We will now fit univariate Poisson regression models on each of the covariates separately and assess their predictive power for the number of doctor visits in the last two years. Likelihood ratio tests for each univariate model compared to a null model are shown in the table below.

```{r}
poireg = function(data, formula) {
  glm(formula, family='poisson', data=data)
}

models = list()
models[[1]] = poireg(data, visit ~ health)
models[[2]] = poireg(data, visit ~ sex)
models[[3]] = poireg(data, visit ~ adldiff)
models[[4]] = poireg(data, visit ~ race)
models[[5]] = poireg(data, visit ~ privins)
models[[6]] = poireg(data, visit ~ age)
models[[7]] = poireg(data, visit ~ cond)
models[[8]] = poireg(data, visit ~ edu)
m0 = poireg(data, visit ~ 1)

predictors = function(model) {
  paste(model$formula)[3]
}

metrics = data.frame(id=1:length(models))
metrics$predictors = sapply(models, predictors)
metrics$loglik = sapply(models, function(m){ lmtest::lrtest(m0, m)[2,'LogLik'] })
metrics$df = sapply(models, function(m){ lmtest::lrtest(m0, m)[2,'Df'] })
metrics$chisq = sapply(models, function(m){ lmtest::lrtest(m0, m)[2,'Chisq'] })
metrics$pvalue = sapply(models, function(m){ lmtest::lrtest(m0, m)[2,'Pr(>Chisq)'] })

metrics %>% mutate_if(is.double, signif, digits=4)
```
We see from the table that all eight covariates achieved p-values less than 0.05, meaning that every covariate increased the likelihood of the model when compared to the null model. The predictors ranked by likelihood are the number of health conditions, the self-reported perception of overall health rating, the ADL disability status, the number of years of education, whether they had private insurance, race, age, and sex.

## Part 4 - Stepwise model building

We will now fit a multivariable model using a backwards stepwise elimination procedure. Since all of the covariates turned out to be significantly associated with the outcome in the univariate models, we will start the stepwise procedure with the full set of covariates.

```{r}
reduced_model <- step(full_model, direction='backward')
summary(reduced_model)
```
The stepwise elimination procedure did not eliminate any variables from the model. As can be verified again from the summary output, all of the predictors were identified as significantly associated with the log of the expected number of doctor visits.

The final estimated regression coefficients are shown in the output above, as well. From this we can predict the estimated ratio of the number of doctor's office visits for those who reported excellent health compared to those who reported poor health. The regression coefficient of -0.578 on the "excellent" level of the health condition variable implies that a person reporting excellent health is expected to have 0.264 times that number of doctor visits in the past two years as someone reporting poor health condition.

```{r}
10**-0.578
```

## Part 5 - Poisson regression diagnostics

Since none of the variables in the full model were eliminated, our "reduced" model is the same as the previously assessed full model. Therefore, the model checks and goodness-of-fit test from earlier still apply. However, we can perform an additional likelihood ratio test comparing the model to a null model.

```{r}
lmtest::lrtest(m0, reduced_model)
```
The null hypothesis is that the models have the same likelihood. The p-value of 2.2e-16 indicates that there is a vanishingly small probability of observing a p-value this large if the null hypothesis were true. Therefore, we reject the null hypothesis and conclude that this set of covariates are associated with the count of doctor visits in the past two years.

We can now investigate whether there are any observations that were predicted poorly, were high leverage, or were highly influential to the model. We can use adjusted residuals to measure poor prediction performance, leverage as a measure of extremity in covariate space, and Cook's distance to measure influential observations. These three diagnostics are plotted against the predicted values in the scatter plots below.

```{r, fig.height=3, fig.width=8.5}

data$predicted = predict(reduced_model, type='response')
data$adjusted_residual = residuals(reduced_model, 'pearson') / sqrt(1 - hatvalues(reduced_model))
data$leverage = hatvalues(reduced_model)
data$cooks_distance = cooks.distance(reduced_model)

plot_rp = data %>%
  ggplot(aes(x=predicted, y=adjusted_residual)) +
  geom_point(color=blue, alpha=0.1) +
  theme_classic() +
  theme(panel.grid.major=element_line(), legend.position='none')

plot_hp = data %>%
  ggplot(aes(x=predicted, y=leverage)) +
  geom_point(color=blue, alpha=0.1) +
  theme_classic() +
  theme(panel.grid.major=element_line(), legend.position='none')

plot_bp = data %>%
  ggplot(aes(x=predicted, y=cooks_distance)) +
  geom_point(color=blue, alpha=0.1) +
  theme_classic() +
  theme(panel.grid.major=element_line(), legend.position='none')

grid.arrange(
  plot_rp, plot_hp, plot_bp,
  ncol=3
)
```

From these plots we can see that the model's predictive performance is subpar, at best. Many of the observations have adjusted residuals greater than 5 or even 10 standard errors, and almost all of these large residuals were positive. This seems to imply that the model severely underestimates the number of hospital visits made by certain individuals who are in the upper tail of the distribution.

There is also evidence of trend between the leverage of an observation and the predicted value, with the leverage increasing as the predicted value increases. Furthermore, many observations are identified as influential by using the $4/n$ rule of thumb for Cook's distance. The severely underestimated counts are also the most influential points in the data set. All of this seems to indicate that the model may be mispecified in some way due to the fact that it predicts poorly and in a consistent pattern.

_